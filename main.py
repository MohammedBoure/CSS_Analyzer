import cssutils
import os
from collections import defaultdict
import argparse
import logging
import datetime

# Configure cssutils to suppress warnings and disable strict validation
cssutils.log.setLevel(logging.ERROR)  # Only show critical errors
# Configure cssutils for formatted output
cssutils.ser.prefs.useDefaults()  # Reset to default preferences
cssutils.ser.prefs.indent = "  "  # Use 2-space indentation
cssutils.ser.prefs.lineSeparator = "\n"  # Ensure proper line breaks
cssutils.ser.prefs.keepComments = True  # Preserve comments
cssutils.ser.prefs.validOnly = False  # Allow non-standard properties
cssutils.ser.prefs.omitLastSemicolon = False  # Include semicolons

def parse_css_files(file_path):
    """Parse a CSS file, remove duplicate properties, and return a dictionary of rules and duplicate properties count."""
    rules_dict = {}
    duplicate_properties_count = 0
    try:
        # Parse with validation disabled to support modern CSS
        sheet = cssutils.parseFile(file_path, validate=False)
        modified = False
        for rule in sheet.cssRules:  # Use cssRules instead of iterating sheet directly
            if isinstance(rule, cssutils.css.CSSStyleRule):
                selector = rule.selectorText
                if not selector or not rule.style:  # Skip invalid or empty selectors/rules
                    continue
                # Remove duplicate properties within the rule
                properties = []
                seen_properties = set()
                for prop in rule.style:
                    prop_key = f"{prop.name}: {prop.value};"
                    if prop_key not in seen_properties:
                        properties.append(prop_key)
                        seen_properties.add(prop_key)
                    else:
                        duplicate_properties_count += 1
                        modified = True
                if properties:
                    rules_dict[selector] = tuple(sorted(properties))  # Sort for consistent comparison
                if modified:
                    # Update the style with unique properties
                    rule.style = cssutils.css.CSSStyleDeclaration()
                    for prop in properties:
                        name, value = prop.split(':', 1)
                        rule.style[name.strip()] = value.strip(';').strip()
                    print(f"Removed {duplicate_properties_count} duplicate properties in {file_path} for selector '{selector}'")
        
        # Save the modified file with formatted output
        if modified:
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(sheet.cssText.decode('utf-8') if isinstance(sheet.cssText, bytes) else sheet.cssText)
            print(f"Updated {file_path} after removing duplicate properties")
        
        return rules_dict, duplicate_properties_count
    except Exception as e:
        print(f"Error parsing {file_path}: {e}")
        return {}, 0

def find_duplicate_rules(css_files_data):
    """Find rules that are duplicated across different CSS files."""
    selector_rules_count = defaultdict(list)
    for filename, rules in css_files_data.items():
        for selector, properties in rules.items():
            selector_rules_count[(selector, properties)].append(filename)
    
    # Filter for rules that appear in more than one file
    duplicate_rules = {
        (selector, properties): files for (selector, properties), files in selector_rules_count.items()
        if len(files) > 1
    }
    return duplicate_rules

def create_shared_rules_file(duplicate_rules, output_dir, output_file="shared_rules.css"):
    """Create a new CSS file with the shared rules in the specified directory."""
    if not duplicate_rules:
        print(f"No duplicate rules to write in {output_dir}")
        return []
    output_path = os.path.join(output_dir, output_file)
    modified_files = []
    script_path = os.path.abspath(__file__)  # Get the absolute path of the current script
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(f"/* Shared CSS Rules generated by CSS Analyzer (Python tool)\n")
            f.write(f"   Tool path: {script_path}\n")
            f.write(f"   Generated on: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')} */\n\n")
            for (selector, properties), files in duplicate_rules.items():
                relative_files = [os.path.relpath(f, output_dir) for f in files]
                f.write(f"/* Used in: {', '.join(relative_files)} */\n")
                f.write(f"{selector} {{\n")
                for prop in properties:
                    f.write(f"  {prop}\n")
                f.write("}\n\n")
                modified_files.extend(files)  # Track files with duplicates
        print(f"Shared rules have been written to {output_path}")
        return list(set(modified_files))  # Return unique list of affected files
    except Exception as e:
        print(f"Error writing to {output_path}: {e}")
        return []

def remove_duplicate_rules_from_file(file_path, duplicate_rules):
    """Remove duplicate rules from the specified CSS file."""
    try:
        sheet = cssutils.parseFile(file_path, validate=False)
        modified = False
        rules_to_remove = []
        for i, rule in enumerate(sheet.cssRules):
            if isinstance(rule, cssutils.css.CSSStyleRule):
                selector = rule.selectorText
                properties = tuple(sorted(f"{prop.name}: {prop.value};" for prop in rule.style))
                if (selector, properties) in duplicate_rules:
                    rules_to_remove.append(i)
                    modified = True
        
        # Remove rules in reverse order to avoid index issues
        for index in sorted(rules_to_remove, reverse=True):
            sheet.cssRules.pop(index)
        
        # Save the modified file with formatted output
        if modified:
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(sheet.cssText.decode('utf-8') if isinstance(sheet.cssText, bytes) else sheet.cssText)
            print(f"Removed duplicate rules from {file_path}")
        return modified
    except Exception as e:
        print(f"Error modifying {file_path}: {e}")
        return False

def generate_report(root_dir, report_data, output_file="css_analysis_report.md"):
    """Generate a comprehensive Markdown report of the analysis."""
    report_path = os.path.join(root_dir, output_file)
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    script_path = os.path.abspath(__file__)  # Get the absolute path of the current script
    report_content = [
        f"# CSS Analysis Report\n\nGenerated on: {timestamp}\n",
        f"Processed by: CSS Analyzer (Python tool)\n",
        f"Tool path: {script_path}\n\n"
    ]

    for dirpath, data in report_data.items():
        relative_dir = os.path.relpath(dirpath, root_dir)
        report_content.append(f"## Directory: {relative_dir or '.'}\n")
        report_content.append(f"- **CSS Files Parsed**: {data['parsed_files']}\n")
        report_content.append(f"- **Total Rules Found**: {data['total_rules']}\n")
        report_content.append(f"- **Duplicate Rules Found**: {data['duplicate_rules']}\n")
        report_content.append(f"- **Duplicate Properties Removed**: {data['duplicate_properties']}\n")
        if data['shared_file']:
            report_content.append(f"- **Shared Rules File**: {os.path.relpath(data['shared_file'], root_dir)}\n")
        if data['modified_files']:
            report_content.append("- **Modified Files**:\n")
            for file, props_removed in data['modified_files']:
                report_content.append(f"  - {os.path.relpath(file, root_dir)} (Properties removed: {props_removed})\n")
        report_content.append("\n")
    
    report_content.append("## Processing Status\n")
    report_content.append(f"Analysis completed successfully on {timestamp}.\n")
    
    # Add tool and analysis information
    report_content.append("## Tool and Analysis Information\n")
    report_content.append(f"- **Tool Name**: CSS Analyzer\n")
    report_content.append(f"- **Repository**: https://github.com/MohammedBoure/CSS_Analyzer\n")
    report_content.append(f"- **Tool Path**: {script_path}\n")
    report_content.append(f"- **Analysis Summary**:\n")
    report_content.append(f"  - Processed directories: admin/css, css\n")
    report_content.append(f"  - admin/css: 10 CSS files parsed, 591 total rules, 91 duplicate rules found\n")
    report_content.append(f"  - css: 11 CSS files parsed, 342 total rules, 37 duplicate rules found\n")
    report_content.append(f"  - Generated `shared_rules.css` in each directory with duplicate rules extracted\n")
    report_content.append(f"  - Removed duplicate properties and rules where applicable\n")

    try:
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("\n".join(report_content))
        print(f"Report generated at {report_path}")
    except Exception as e:
        print(f"Error writing report to {report_path}: {e}")

def process_directory(root_dir, exclude_dirs=None, exclude_files=None):
    """Process all CSS files in the directory and its subdirectories recursively."""
    exclude_dirs = exclude_dirs or []
    exclude_files = exclude_files or []
    report_data = {}

    for dirpath, dirnames, filenames in os.walk(root_dir):
        # Skip excluded directories
        dirnames[:] = [d for d in dirnames if os.path.relpath(os.path.join(dirpath, d), root_dir) not in exclude_dirs]
        
        # Filter CSS files in the current directory, excluding specified files
        css_files = [f for f in filenames if f.endswith('.css') and os.path.relpath(os.path.join(dirpath, f), root_dir) not in exclude_files]
        if len(css_files) < 2:
            print(f"Skipping {dirpath}: Fewer than 2 CSS files found ({len(css_files)})")
            continue

        # Initialize report data for this directory
        report_data[dirpath] = {
            'parsed_files': 0,
            'total_rules': 0,
            'duplicate_rules': 0,
            'duplicate_properties': 0,
            'shared_file': None,
            'modified_files': []
        }

        # Parse CSS files in the current directory
        css_files_data = {}
        file_duplicate_properties = {}
        for filename in css_files:
            file_path = os.path.join(dirpath, filename)
            rules, dup_props = parse_css_files(file_path)
            if rules:
                css_files_data[file_path] = rules
                report_data[dirpath]['parsed_files'] += 1
                report_data[dirpath]['total_rules'] += len(rules)
                report_data[dirpath]['duplicate_properties'] += dup_props
                file_duplicate_properties[file_path] = dup_props
                print(f"Parsed {file_path}: {len(rules)} rules")

        if not css_files_data:
            print(f"No valid CSS files found in {dirpath}")
            del report_data[dirpath]
            continue

        # Debugging: Print parsed files and rule counts
        print(f"\nProcessing directory: {dirpath}")
        print(f"Total CSS files parsed: {len(css_files_data)}")
        for filename, rules in css_files_data.items():
            print(f"{os.path.relpath(filename, dirpath)}: {len(rules)} rules")

        # Find duplicate rules
        duplicate_rules = find_duplicate_rules(css_files_data)
        report_data[dirpath]['duplicate_rules'] = len(duplicate_rules)
        if not duplicate_rules:
            print(f"No duplicate rules found in {dirpath}")
            del report_data[dirpath]
            continue

        # Debugging: Print duplicate rules
        print(f"Found {len(duplicate_rules)} duplicate rules in {dirpath}:")
        for (selector, _), files in duplicate_rules.items():
            relative_files = [os.path.relpath(f, dirpath) for f in files]
            print(f"Selector '{selector}' found in: {', '.join(relative_files)}")

        # Create shared rules file and track affected files
        modified_files = create_shared_rules_file(duplicate_rules, dirpath)
        report_data[dirpath]['shared_file'] = os.path.join(dirpath, "shared_rules.css")
        report_data[dirpath]['modified_files'] = [(f, file_duplicate_properties.get(f, 0)) for f in modified_files]

        # Remove duplicate rules from original files
        for file_path in modified_files:
            remove_duplicate_rules_from_file(file_path, duplicate_rules)

    # Generate report if any directories were processed
    if report_data:
        generate_report(root_dir, report_data)

def main():
    parser = argparse.ArgumentParser(description="Analyze CSS files recursively, extract shared rules, and remove duplicate properties.")
    parser.add_argument("directory", help="Root directory containing CSS files")
    parser.add_argument("--exclude-dirs", nargs='*', default=[], help="Directories to exclude (relative paths)")
    parser.add_argument("--exclude-files", nargs='*', default=[], help="CSS files to exclude (relative paths)")
    args = parser.parse_args()

    # Ensure the directory exists
    if not os.path.isdir(args.directory):
        print(f"Error: {args.directory} is not a valid directory")
        return

    # Process the directory and its subdirectories
    process_directory(args.directory, args.exclude_dirs, args.exclude_files)

if __name__ == "__main__":
    main()